{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Advanced)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will\n",
    "\n",
    "- Code our Gradient Descent in vectorized form for a high-dimensional Loss Function\n",
    "- Fine-tune your choice of # of epochs on GD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to study the [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) and try to predict the **intensity of the disease** based on **10 quantitative features**, such as body-mass-index, age, etc. (regression problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.load_diabetes(return_X_y = True, as_frame = True)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y, kde = True);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code a Vectorial Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're modeling a linear regression $\\hat{y} = X\\beta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/vectorial-gradient.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first, let's add an \"intercept\" column of \"ones\" to our feature matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add an intercept column of \"ones\" \n",
    "X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a train/test split for you with `test_size=0.3` and `random_state=1` (so that we all have repeatable results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's recall the definition of the gradient descent algorithm\n",
    "\n",
    "$$\\text{Gradient descent - vector formula}$$\n",
    "$$\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\beta^{\\color{red}{(k)}})$$\n",
    "\n",
    "The MSE Loss for an OLS regression is\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n}\\|X \\beta - y\\|^2 = \\frac{1}{n}(X \\beta - y)^T(X \\beta - y)$$\n",
    "\n",
    "and its gradient is\n",
    "$${\\displaystyle \\nabla L(\\beta)=\n",
    "{\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\beta_{0}}}(\\beta)\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\beta_{p}}}(\\beta)\\end{bmatrix}} = \\frac{2}{n} X^T (X\\beta - y) \n",
    "}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store our main problem parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n observations\n",
    "n = X.shape[0] \n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# p features (including the intercept)\n",
    "p = X.shape[1]\n",
    "\n",
    "# Gradient Descent hyper-params\n",
    "eta = .1\n",
    "n_epochs= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Initialize a $\\beta$ vector of zeros of shape **p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using the vectorized formula given above, create a Gradient Descent that loops over `n_epochs` to find the best $\\beta$ of an OLS using the `train` set\n",
    "- make use of NumPy's matrix operations and broadcasting capabilities\n",
    "- this shouldn't take more than 4 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    gradient = 2 / n_train * np.dot(X_train.T, (np.dot(X_train, beta) - y_train))\n",
    "    beta = beta - eta * gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best √ü: ', beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCompute predictions on your test set (`y_pred`), and the resulting `loss_test` (MSE loss for OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap these into a function called `gradient_decent`\n",
    "\n",
    "‚ùì Wrap this logic into a function called `gradient_descent`, which takes as input some (`X_train`, `y_train`, `X_test`, `y_test`, `eta`, `n_epoch`) values, and returns:\n",
    "- the final value for $\\beta$ fitted on the train set\n",
    "- the values of the `loss_train` at each epoch as a list called `loss_train_history`\n",
    "- the values of the `loss_test` at each epoch as a list called `loss_test_history`\n",
    "- (optional) make the function robust to call with only a train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_test, y_test, eta = eta, n_epochs = 100):\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "\n",
    "    beta = np.zeros(p)\n",
    "\n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "\n",
    "    pass  # YOUR CODE HERE\n",
    "\n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping criteria?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìPlot the loss as a function of epochs, on your train dataset. \n",
    "- Try it with `n_epochs=10000` and `eta=0.1` as was initially set\n",
    "- Zoom in with `plt.ylim(ymin=2800, ymax=3000)` to see the behavior of the Loss Function on the test set\n",
    "- What can you conclude? Should you always \"descend\" the gradient down to the absolute minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and test histories\n",
    "plt.plot(loss_train_history, label = 'loss_train')\n",
    "plt.plot(loss_test_history, label = 'loss_test')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Loss')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Change limits\n",
    "plt.ylim(ymin = 2800, ymax = 3000)\n",
    "\n",
    "# Generate legend\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì What do you notice?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCan you think of a method to improve the performance of your model? Take time to write it in pseudo-code below before looking at the hints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- We could decide to stop the GD as soon as the test loss starts to increase again.\n",
    "- ‚ö†Ô∏è Yet we can't use the \"test set\" created initially to decide when to stop descending gradient; this would create data leakage! Never use your test set to optimize your model's `hyperparameters`.\n",
    "- Create instead a train/test split **within** your current training set and optimize your early stopping based on the loss of this new test set only. This one is usually called a **validation set**. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Update your `gradient_descent` method based on the Hints above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Create your train/val set and try to improve your MSE with early stopping, using `random_state = 1`\n",
    "\n",
    "It should stop earlier than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_es, loss_train_history, loss_val_history = gradient_descent_early_stopping(X_train_train, y_train_train, X_val, y_val, n_epochs = 10000, eta = .1)\n",
    "\n",
    "# Plot train and test histories\n",
    "plt.plot(loss_train_history, label = 'loss_train')\n",
    "plt.plot(loss_test_history, label = 'loss_test')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title('Loss')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.xlabel('Epochs')\n",
    "\n",
    "# Change limits\n",
    "plt.ylim(ymin = 2500, ymax = 4000)\n",
    "\n",
    "# Generate legend\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Modify your gradient_descent function into a `minibatch_gradient_descent` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X_train, y_train, X_test, y_test, batch_size = 16, eta = eta, n_epochs = n_epochs):\n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "\n",
    "    p = X_train.shape[1]\n",
    "\n",
    "    beta = np.zeros(p)\n",
    "\n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "\n",
    "    if isinstance(y_train, pd.Series):\n",
    "        y_train = y_train.to_numpy()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Shuffle your (X_train, y_train) dataset\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        # Loop over your dataset in mini-batches, and for each mini-batch update your beta\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "        # Keep track of loss histories per epoch\n",
    "        pass  # YOUR CODE HERE\n",
    "\n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Plot the evolution of your train and val losses per epoch. What if you chose minibatch = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì How would you adjust the early stopping criteria to these fluctuations?\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "To avoid early stopping too early due to the stochastic nature of the mini-batch descent, we could add a \"patience\" term to stop only after the val loss is increased for a sustained period of \"patience\" # of epochs.\n",
    "</details>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: a new way to check for overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://wagon-public-datasets.s3-eu-west-1.amazonaws.com/05-Machine-Learning/04-Under-the-Hood/underfitting_overfitting_at_a_glance.webp\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Read more about:\n",
    "\n",
    "- [Underfitting and overfitting](https://towardsdatascience.com/overfitting-vs-underfitting-a-complete-example-d05dd7e19765)\n",
    "- [Gradient Descent in Python](https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ Congrats on completing this challenge! Time to push your notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
